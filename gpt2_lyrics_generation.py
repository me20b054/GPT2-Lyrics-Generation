# -*- coding: utf-8 -*-
"""Gpt2 Lyrics Generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tMzILEzrcaap-lXJJFJOBUQKZuuINvir
"""

pip install transformers

import pandas as pd
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import numpy as np
import random
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import torch.nn.functional as F
import os

from google.colab import drive
drive.mount('/content/drive')

### Prepare data
lyrics = pd.read_csv('/content/drive/MyDrive/Files_for_tuning/lyrics-data.csv') # /content/lyrics-data.csv
lyrics[:5]

lyrics = lyrics[lyrics['language']=='en']

#Only keep popular artists, with genre Rock/Pop and popularity high enough
artists = pd.read_csv('/content/drive/MyDrive/Files_for_tuning/artists-data.csv')
artists = artists[(artists['Genres'].isin(['Rock'])) & (artists['Popularity']>5)]
df = lyrics.merge(artists[['Artist', 'Genres', 'Link']], left_on='ALink', right_on='Link', how='inner')
df = df.drop(columns=['ALink','SLink','language','Link'])

len(artists)

len(df)

df.head()

token_lengths = df["Lyric"].apply(lambda x: len(x.split(" ")))
#max(token_lengths)
token_lengths

type(token_lengths)

df = df[df["Lyric"].apply(lambda x: len(x.split(" ")) < 350)]

df.head()

len(df.iloc[0].Lyric.split(" "))

t = df.loc[1:6,["SName","Lyric"]]
t

type(t)

m = df.iloc[3]
m

data = {
    "SName": ["I'm The One","I'm not the one"],
    "Lyric": ["Ah-hah!\nWoo!\nAh-ha-ha-ha-ha-ha!\nWe came here to say goo...","Say hello"],
    "Artist": ["4 Non Blondes","4 Non Blondes"],
    "Genres": ["Rock","Rock"]
}

# Convert the dictionary to a DataFrame with first row as column labels
f = pd.DataFrame(data, columns=data.keys())

f

d = pd.DataFrame(m)
d

#Create a very small test set to compare generated text with the reality
test_set = df.sample(n = 5)

#takes only the indices of original df which are not in test_set (negation is used for that)
df = df.loc[~df.index.isin(test_set.index)]

#Reset the indexes
test_set = test_set.reset_index()
df = df.reset_index()

df.head()

len(test_set)

test_set[:5]

#For the test set only, keep last 20 words in a new column, then remove them from original column
test_set['True_end_lyrics'] = test_set['Lyric'].str.split().str[-20:].apply(' '.join)
# Above can be done like this
# test_set['True_end_lyrics'] = " ".join(test_set["Lyric"].split(" ")[-20:])
test_set['Lyric'] = test_set['Lyric'].str.split().str[:-20].apply(' '.join)

n = test_set.Lyric[2]

#array
a = n.split()[-20:]
j = " ".join(a)
j

class SongLyrics(Dataset):
    def __init__(self, control_code, truncate=False, gpt2_type="gpt2", max_length=1024):

        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)
        self.lyrics = []

        for row in df['Lyric']:
          self.lyrics.append(torch.tensor(
                self.tokenizer.encode(f"<|{control_code}|>{row[:max_length]}<|endoftext|>")
            ))
        if truncate:
            self.lyrics = self.lyrics[:20000]
        self.lyrics_count = len(self.lyrics)

    def __len__(self):
        return self.lyrics_count

    def __getitem__(self, item):
        return self.lyrics[item]

dataset = SongLyrics(df['Lyric'], truncate=True, gpt2_type="gpt2")

#Get the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

#Accumulated batch size (since GPT2 is so big)
def pack_tensor(new_tensor, packed_tensor, max_seq_len):
    if packed_tensor is None:
        return new_tensor, True, None
    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:
        return packed_tensor, False, new_tensor
    else:
        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)
        return packed_tensor, True, None

def train(
    dataset, model, tokenizer,
    batch_size=16, epochs=20, lr=2e-5,
    max_seq_len=400, warmup_steps=200,
    gpt2_type="gpt2", output_dir=".", output_prefix="wreckgar",
    test_mode=False,save_model_on_epoch=False,
):
    acc_steps = 100
    device=torch.device("cuda")
    model = model.cuda()
    model.train()

    optimizer = AdamW(model.parameters(), lr=lr)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1
    )

    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    loss=0
    accumulating_batch_count = 0
    input_tensor = None

    for epoch in range(epochs):

        print(f"Training epoch {epoch}")
        print(loss)
        for idx, entry in tqdm(enumerate(train_dataloader)):
            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)

            if carry_on and idx != len(train_dataloader) - 1:
                continue

            input_tensor = input_tensor.to(device)
            outputs = model(input_tensor, labels=input_tensor)
            loss = outputs[0]
            loss.backward()

            if (accumulating_batch_count % batch_size) == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                model.zero_grad()

            accumulating_batch_count += 1
            input_tensor = None

        print(loss)

        if save_model_on_epoch:
            torch.save(
                model.state_dict(),
                os.path.join(output_dir, f"{output_prefix}-{epoch}.pt"),
            )
    return model

"""# New Section"""

model = train(dataset, model, tokenizer)

# torch.save(model,"/content/")
def generate(
    model,
    tokenizer,
    prompt,
    entry_count=10,
    entry_length=30, #maximum number of words
    top_p=0.8,
    temperature=1.,
):
    model.eval()
    generated_num = 0
    generated_list = []

    filter_value = -float("Inf")

    with torch.no_grad():

        for entry_idx in trange(entry_count):

            entry_finished = False
            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)

            for i in range(entry_length):
                outputs = model(generated, labels=generated)
                loss, logits = outputs[:2]
                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)

                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[
                    ..., :-1
                ].clone()
                sorted_indices_to_remove[..., 0] = 0

                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[:, indices_to_remove] = filter_value

                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)
                generated = torch.cat((generated, next_token), dim=1)

                if next_token in tokenizer.encode("<|endoftext|>"):
                    entry_finished = True

                if entry_finished:

                    generated_num = generated_num + 1

                    output_list = list(generated.squeeze().numpy())
                    output_text = tokenizer.decode(output_list)
                    generated_list.append(output_text)
                    break

            if not entry_finished:
              output_list = list(generated.squeeze().numpy())
              output_text = f"{tokenizer.decode(output_list)}<|endoftext|>"
              generated_list.append(output_text)

    return generated_list

#Function to generate multiple sentences. Test data should be a dataframe
def text_generation(test_data):
  generated_lyrics = []
  for i in range(len(test_data)):
    x = generate(model.to('cpu'), tokenizer, test_data['Lyric'][i], entry_count=1)
    generated_lyrics.append(x)
  return generated_lyrics

def one_generation(text):
  return generate(model,tokenizer,text,entry_count=1)

#Run the functions to generate the lyrics
generated_lyrics = text_generation(test_set)

test_set.loc[3,"Lyric"]

one_generation('I bought a bourgeois house in the Hollywood hills With a truckload of hundred thousand dollar bills Man came by to hook up my cable TV We settled in for the night my baby and me We switched \'round and \'round \'til half-past dawn There was fifty-seven channels and nothin\' on Well now home entertainment was my baby\'s wish So I hopped into town for a satellite dish I tied it to the top of my Japanese car I came home and I pointed it out into the stars A message came back from the great beyond There\'s fifty-seven channels and nothin\' on Well we might\'a made some friends with some billionaires We might\'a got all nice and friendly if we\'d made it upstairs All I got was a note that said "Bye-bye John Our love is fifty-seven channels and nothin\' on" So I bought a .44 magnum it was solid steel cast And in the blessed name of Elvis well I just let it blast \'Til my TV lay in pieces there at my feet And they busted me for disturbing the almighty peace Judge said "What you got in your defense son?" "Fifty-seven')

test_set["True_end_lyrics"][3]

test_set["Generated_lyrics"][3]

#Loop to keep only generated text and add it as a new column in the dataframe
my_generations=[]

for i in range(len(generated_lyrics)):
  a = test_set['Lyric'][i].split()[-30:] #Get the matching string we want (30 words)
  b = ' '.join(a)
  c = ' '.join(generated_lyrics[i]) #Get  all that comes after the matching string
  my_generations.append(c.split(b)[-1])

test_set['Generated_lyrics'] = my_generations

print(b)
print(c)
print(my_generations)
#Finish the sentences when there is a point, remove after that
# final=[]

# for i in range(len(test_set)):
#   to_remove = test_set['Generated_lyrics'][i].split('.')[-1]
#   final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))

# test_set['Generated_lyrics'] = final

test_set

#



print(test_set["True_end_lyrics"][2])
print(test_set["Generated_lyrics"][2])

# Using BLEU score to compare the real sentences with the generated ones
# import statistics
# from nltk.translate.bleu_score import sentence_bleu

# scores=[]

# for i in range(len(test_set)):
#   reference = test_set['True_end_lyrics'][i]
#   candidate = test_set['Generated_lyrics'][i]
#   scores.append(sentence_bleu(reference, candidate))

# print(scores)
# #statistics.mean(scores)



